from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from pandas import read_csv, DataFrame
from os.path import join

seed = 42
dataPath = r"D:\Project\Sem7\DataPrepared\data0Prepared.csv"

# Meta-heuristic constants 
eph = 1e3
earlyStop = 10
N = 10


def splitData(features, label):
    global xTrain, xTest, yTrain, yTest, xVal, yVal
    xTrain, xTest, yTrain, yTest = train_test_split(features, label, test_size=0.2, random_state=seed)
    xTrain, xVal, yTrain, yVal = train_test_split(xTrain, yTrain, test_size=0.25, random_state=seed)


def showMetrics(yPred, modelName):
    yPred = [ele > 0.50 for ele in yPred]
    print(modelName)
    acc = round(accuracy_score(yTest, yPred), 4)
    prsc = round(precision_score(yTest, yPred), 4)
    rcll = round(recall_score(yTest, yPred), 4)
    f1 = round(f1_score(yTest, yPred), 4)
    roc = round(roc_auc_score(yTest, yPred), 4)
    print('Confusion Matrix:\n', confusion_matrix(yTest, yPred))
    print('Accuracy Score:', acc)
    print('Precision Score:', prsc)
    print('Recall Score:', rcll)
    print('F1 Score:', f1)
    print('ROC AUC score:', roc)
    return (acc, prsc, rcll, f1, roc)

    
# Lower and Upper bound
#        0     1     2    3    4     5   
lb = [ 150,    0,    0, 1e3, 1e4,    0]
ub = [ 250, 2.99, 3.99, 1e5, 1e5, 7.99]

# Encoders
weights = {0: 'uniform', 1: 'distance', 2: None}
algorithm = {0: 'auto', 1: 'ball_tree', 2: 'kd_tree', 3: 'brute'}
metric = {0: 'minkowski', 1: 'cityblock', 2: 'cosine', 3: 'euclidean', 4: 'l1', 5: 'l2', 6: 'manhattan', 7: 'nan_euclidean'}


def makeHyperparameters(sol):
    # print(sol)
    nNeighbors = int(sol[0])
    weightsEnc = int(sol[1])
    wghs = weights[weightsEnc]
    algorithmEnc = int(sol[2])
    algo = algorithm[algorithmEnc]
    leafSize = int(sol[3])
    p = int(sol[4])
    metricEnc = int(sol[5])
    met = str(metric[metricEnc])
    params = {
        'n_neighbors': nNeighbors,
        'weights': wghs,
        'algorithm': algo,
        'leaf_size': leafSize,
        'p': p,
        'metric': met
}
    # print(params)
    return params


def fitness(sol):
    hyperparameters = makeHyperparameters(sol)
    try:
        regg = KNeighborsClassifier(**hyperparameters)
        regg.fit(X=xTrain, y=yTrain)
    except:
        return 0
    else:
        yP = regg.predict(X=xVal)
        roc = roc_auc_score(yVal, yP)
        return roc
        # r2 = mt.r2_score(yVal, yP)
        # return r2
    

def metaheuristicHypertune(optm):
    '''Hypertuning of parameters of K Neighbors Classification'''

    prb = {
        'fit_func': fitness,
        'lb': lb,
        'ub': ub,
        'minmax': 'max'
    }
    trmn = {
        'max_early_stop': earlyStop
    }

    try:    
        sol, bestFit = optm.solve(problem=prb, termination=trmn)
        # print('Solution:\n', sol)
        #Best model evaluation
        bestHyperparameters = makeHyperparameters(sol)
        bestModel = KNeighborsClassifier(**bestHyperparameters)
        bestModel.fit(X=xTrain, y=yTrain)
    except Exception as e:
        bestHyperparameters = e
        metrics = (0, 0, 0, 0, 0)
        bestFit = 0
    else:         
        yPred = bestModel.predict(X=xTest)
        metrics = showMetrics(yPred, 'K Neighbors Classifier Optimised')
        print('Best Fit:\n', bestFit)

    return bestHyperparameters, metrics, bestFit


def bayesOpt(n_neighbors, weights, algorithm, leaf_size, p, metric,):
    sol = [n_neighbors, weights, algorithm, leaf_size, p, metric,]
    params = makeHyperparameters(sol)

    model = KNeighborsClassifier(**params)
    
    try:
        model.fit(xTrain, yTrain)
        pred = model.predict(xVal)
        return roc_auc_score(yVal, pred)
    except:
        return 0


def bayesOptimisation():
    from bayes_opt import BayesianOptimization
    params = {
        'n_neighbors': (lb[0], ub[0]),
        'weights': (lb[1], ub[1]),
        'algorithm': (lb[2], ub[2]),
        'leaf_size': (lb[3], ub[3]),
        'p': (lb[4], ub[4]),
        'metric': (lb[5], ub[5]),
    }

    optimise = BayesianOptimization(bayesOpt, params, random_state=seed)
    optimise.maximize(n_iter=10, init_points=10)

    bestParams = optimise.max['params']
    n_neighbors = bestParams['n_neighbors']
    weights = bestParams['weights']
    algorithm = bestParams['algorithm']
    leaf_size = bestParams['leaf_size']
    p =  bestParams['p']
    metric = bestParams['metric']
    sol = [n_neighbors, weights, algorithm, leaf_size, p, metric,]
    bestParams = makeHyperparameters(sol)

    print(bestParams)
    regg = KNeighborsClassifier(**bestParams)
    try:
        regg.fit(xTrain, yTrain)
        yPred = regg.predict(xTest)
        metrics = showMetrics(yPred, 'KNC Optimised by Bayes Optimisation')
    except:
        metrics = (0, 0, 0, 0, 0)
    return bestParams, metrics


def PowerSet(s, n): 
    '''Function to print the power set'''
    def findPowerSet(s, res, n): 
        if (n == 0): 
            temp = []
            for i in res: 
                temp.append(i) 
            columnsPowerSet.append(temp)
            return
    
        # append the subset to result 
        res.append(s[n - 1]) 
        findPowerSet(s, res, n - 1) 
        res.pop() 
        findPowerSet(s, res, n - 1) 

    ans = [] 
    findPowerSet(s, ans, n)



def columnInclusion(optName='BayesOpt'):
    '''Hyper tuning by including columns'''

    Column = ['Latitude', 'Longitude', 'TWI', 'TPI', 'Slope', 'Rainfall', 'Hillshade', 'Elevation', 'DrainageDensity', 
              'ProfileCurvature', 'PlanCurvature', 'Curvature', 'Aspect', 'Soil', 'LULC']
    
    dfFileName = f'columnInclusionFeatsSelect{optName}KNC.csv'
    dfPath = join(r"D:\Project\Sem7\Python\reportFiles", dfFileName)
    columnsPowerSet = []
    PowerSet(Column[:], len(Column[:]))
    columnsPowerSet.pop()
    for i in reversed(range(0, len(columnsPowerSet))):
        data = read_csv(dataPath)
        try:
            columnsPowerSet[i].remove('Soil')
            columnsPowerSet[i] += ['Soil3639', 'Soil3662', 'Soil3691', 'Soil3717', 'Soil3761', 'Soil3849', 'Soil3850', 'Soil6998']
        except:
            pass
        try: 
            columnsPowerSet[i].remove('LULC')
            columnsPowerSet[i] += ['LULC1', 'LULC2', 'LULC5', 'LULC7', 'LULC8', 'LULC9', 'LULC10', 'LULC11']
        except:
            pass
        try:
            df = read_csv(dfPath)
        except:
            df = DataFrame(columns=['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc', 'includedColumns', 'params', ])
        print('Included columns:\n', columnsPowerSet[i])
        features = data.loc[:, columnsPowerSet[i]]
        label = data.iloc[:, -1]
        splitData(features, label)

        if optName.lower()[0] == 'b':
            params, metrics = bayesOptimisation()
            df.loc[i, 'includedColumns'] = str(columnsPowerSet[i])
            df.loc[i, 'params'] = str(params)
            df.loc[i, 'accuracy'] = metrics[0]
            df.loc[i, 'precision'] = metrics[1]
            df.loc[i, 'recall'] = metrics[2]
            df.loc[i, 'f1_score'] = metrics[3]
            df.loc[i, 'roc_auc'] = metrics[4]
        elif optName.lower()[0] == 'm':
            from metaHeuristicOptimizers import getOptimizersDictionary
            dictOpt = getOptimizersDictionary(epochs=eph, population=N)
            for k in range(0, len(dictOpt.keys())):
                print('\n\nHeuristic k: ', k)
                key = list(dictOpt.keys())[k]
                params, metrics, bestFit = metaheuristicHypertune(dictOpt[key])
                df.loc[i, 'includedColumns'] = str(columnsPowerSet[i])
                df.loc[i, 'meteHeuristic'] = str(key)
                df.loc[i, 'params'] = str(params)
                df.loc[i, 'accuracy'] = metrics[0]
                df.loc[i, 'precision'] = metrics[1]
                df.loc[i, 'recall'] = metrics[2]
                df.loc[i, 'f1_score'] = metrics[3]
                df.loc[i, 'roc_auc'] = metrics[4]
                df.loc[i, 'bestFit'] = bestFit

        df.to_csv(dfPath, index=False)


def columnExclusion(optName='BayesOpt'):
    '''Hyper tuning by removing columns'''

    Column = ['Latitude', 'Longitude', 'TWI', 'TPI', 'Slope', 'Rainfall', 'Hillshade', 'Elevation', 'DrainageDensity', 
              'ProfileCurvature', 'PlanCurvature', 'Curvature', 'Aspect', 'Soil', 'LULC']
    excludeClmList = ['Curvature', 'ProfileCurvature', 'PlanCurvature']
    for t in excludeClmList:
        Column.remove(t)
    dfFileName = f'columnExclusionFeatsSelect{optName}KNC.csv'
    dfPath = join(r"D:\Project\Sem7\Python\reportFiles", dfFileName)
    itStart = 0
    kStart = 0
    for it in range(itStart, len(Column)):
        item = Column[it]
        data = read_csv(dataPath)
        clm = [item, ] + excludeClmList
        try:
            df = read_csv(dfPath)
            i = len(df)
        except:
            df = DataFrame(columns=['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc', 'excludedColumns', 'params', ])
            i = 0
        if item == 'LULC':
            clm = excludeClmList + ['LULC1', 'LULC2', 'LULC5', 'LULC7', 'LULC8', 'LULC9', 'LULC10', 'LULC11']
        if item == 'Soil':
            clm = excludeClmList + ['Soil3639', 'Soil3662', 'Soil3691', 'Soil3717', 'Soil3761', 'Soil3849', 'Soil3850', 'Soil6998']
        data = data.drop(clm, axis=1)
        features = data.iloc[:, :-1]
        label = data.iloc[:, -1]
        splitData(features, label)

        if optName.lower()[0] == 'b':
            print('Excluded columns, it: \n', clm, it)
            params, metrics = bayesOptimisation()
            df.loc[i, 'excludedColumns'] = str(clm)
            df.loc[i, 'params'] = str(params)
            df.loc[i, 'accuracy'] = metrics[0]
            df.loc[i, 'precision'] = metrics[1]
            df.loc[i, 'recall'] = metrics[2]
            df.loc[i, 'f1_score'] = metrics[3]
            df.loc[i, 'roc_auc'] = metrics[4]
            df.to_csv(dfPath, index=False)
            i += 1
        elif optName.lower()[0] == 'm':
            from metaHeuristicOptimizers import getOptimizersDictionary
            dictOpt = getOptimizersDictionary(epochs=eph, population=N)
            for k in range(kStart, len(dictOpt.keys())):
                print('\n\nHeuristic k: ', k)
                print('Excluded columns, it: \n', clm, it)
                key = list(dictOpt.keys())[k]
                params, metrics, bestFit = metaheuristicHypertune(dictOpt[key])
                df.loc[i, 'excludedColumns'] = str(clm)
                df.loc[i, 'meteHeuristic'] = str(key)
                df.loc[i, 'params'] = str(params)
                df.loc[i, 'accuracy'] = metrics[0]
                df.loc[i, 'precision'] = metrics[1]
                df.loc[i, 'recall'] = metrics[2]
                df.loc[i, 'f1_score'] = metrics[3]
                df.loc[i, 'roc_auc'] = metrics[4]
                df.loc[i, 'bestFit'] = bestFit
                df.to_csv(dfPath, index=False)
                i += 1
        kStart = 0
        


def normalKNC(optType='bayes'):
    '''Normal check of K Neighbors Classification'''

    data = read_csv(dataPath)
    features = data.iloc[:, :-1]
    label = data.iloc[:, -1]
    splitData(features, label)

    if optType == 'bayes':
        params, metrics = bayesOptimisation(nIter=40)
    elif optType == 'metaheuristic':
        params, metrics = metaheuristicHypertune()

    print('Optimised Parameters: \n', params)



if __name__ == '__main__':
    import time
    t0 = time.time()
    global columnsPowerSet 

    
    # normalKNC('metaheuristic')
    columnExclusion('MetaHeuristic')


    t1 = time.time()
    t = time.strftime('%H:%M:%S', time.gmtime(t1-t0))
    print('Total time taken: ', t)